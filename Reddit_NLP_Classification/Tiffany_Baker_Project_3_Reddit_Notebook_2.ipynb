{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ran 2 instances at the same time (parallel power) to acquire data from my subreddits. This instance also saves off the top 500 strings in the tokens to compare with the top 500 strings in the tokens from the other subreddit. \n",
    "The 2 top 500 lists will be used to create a custom stopword list to further preprocess the corpus before modeling and predicting. \n",
    "\n",
    "The processes in this notebook mirror the processes in Notebook 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve data online and load\n",
    "import requests as req \n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "#NLP libraries\n",
    "from nltk.stem import WordNetLemmatizer as lemma\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, regexp_tokenize, RegexpTokenizer, TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "#Variables to use for NLP\n",
    "stop = stopwords.words('english')\n",
    "punc = \"?!,.;:)(\"\n",
    "\n",
    "#Pandas\n",
    "import pandas as pd\n",
    "\n",
    "#Plot and model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to request data using pushshift\n",
    "#takes\n",
    "def so_fresh(n,sub,post_type):\n",
    "    '''Takes in a number n, name of the subreddit(all lowercase), and post_type(either submissions, comments, or subreddit).\n",
    "    Creates a request url to obtain data through the pushshift api from reddit.\n",
    "    Returns a list of n dataframes comprised of 100 reddit post/comments.\n",
    "    \n",
    "    example: sub = sub_pull(100,\"prorevenge\",\"submission\") \n",
    "    will run 100 iterations of the function to obtain posts from the prorevenge subreddit'''\n",
    "    \n",
    "    df_1 = []\n",
    "    start = 1600822671\n",
    "    for i in range(n):\n",
    "        red_url = f\"https://api.pushshift.io/reddit/search/{post_type}/?subreddit={sub}&before={start}&size=100\"\n",
    "        reqs = req.get(red_url, timeout = 30)\n",
    "        stream = reqs.json()\n",
    "        body = stream[\"data\"]\n",
    "        df = pd.DataFrame(body)[['selftext','subreddit','created_utc']]\n",
    "        start=df['created_utc'].min()\n",
    "        df_1.append(df)\n",
    "        time.sleep(30)\n",
    "        print(f\"Pull {i} complete\") #Chuck suggested to Tanner- incorporated here\n",
    "  \n",
    "    return df_1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pull 0 complete\n",
      "Pull 1 complete\n",
      "Pull 2 complete\n",
      "Pull 3 complete\n",
      "Pull 4 complete\n",
      "Pull 5 complete\n",
      "Pull 6 complete\n",
      "Pull 7 complete\n",
      "Pull 8 complete\n",
      "Pull 9 complete\n",
      "Pull 10 complete\n",
      "Pull 11 complete\n",
      "Pull 12 complete\n",
      "Pull 13 complete\n",
      "Pull 14 complete\n",
      "Pull 15 complete\n",
      "Pull 16 complete\n",
      "Pull 17 complete\n",
      "Pull 18 complete\n",
      "Pull 19 complete\n",
      "Pull 20 complete\n",
      "Pull 21 complete\n",
      "Pull 22 complete\n",
      "Pull 23 complete\n",
      "Pull 24 complete\n",
      "Pull 25 complete\n",
      "Pull 26 complete\n",
      "Pull 27 complete\n",
      "Pull 28 complete\n",
      "Pull 29 complete\n",
      "Pull 30 complete\n",
      "Pull 31 complete\n",
      "Pull 32 complete\n",
      "Pull 33 complete\n",
      "Pull 34 complete\n",
      "Pull 35 complete\n",
      "Pull 36 complete\n",
      "Pull 37 complete\n",
      "Pull 38 complete\n",
      "Pull 39 complete\n",
      "Pull 40 complete\n",
      "Pull 41 complete\n",
      "Pull 42 complete\n",
      "Pull 43 complete\n",
      "Pull 44 complete\n",
      "Pull 45 complete\n",
      "Pull 46 complete\n",
      "Pull 47 complete\n",
      "Pull 48 complete\n",
      "Pull 49 complete\n",
      "Pull 50 complete\n",
      "Pull 51 complete\n",
      "Pull 52 complete\n",
      "Pull 53 complete\n",
      "Pull 54 complete\n",
      "Pull 55 complete\n",
      "Pull 56 complete\n",
      "Pull 57 complete\n",
      "Pull 58 complete\n",
      "Pull 59 complete\n",
      "Pull 60 complete\n",
      "Pull 61 complete\n",
      "Pull 62 complete\n",
      "Pull 63 complete\n",
      "Pull 64 complete\n",
      "Pull 65 complete\n",
      "Pull 66 complete\n",
      "Pull 67 complete\n",
      "Pull 68 complete\n",
      "Pull 69 complete\n",
      "Pull 70 complete\n",
      "Pull 71 complete\n",
      "Pull 72 complete\n",
      "Pull 73 complete\n",
      "Pull 74 complete\n",
      "Pull 75 complete\n",
      "Pull 76 complete\n",
      "Pull 77 complete\n",
      "Pull 78 complete\n",
      "Pull 79 complete\n",
      "Pull 80 complete\n",
      "Pull 81 complete\n",
      "Pull 82 complete\n",
      "Pull 83 complete\n",
      "Pull 84 complete\n",
      "Pull 85 complete\n",
      "Pull 86 complete\n",
      "Pull 87 complete\n",
      "Pull 88 complete\n",
      "Pull 89 complete\n",
      "Pull 90 complete\n",
      "Pull 91 complete\n",
      "Pull 92 complete\n",
      "Pull 93 complete\n",
      "Pull 94 complete\n",
      "Pull 95 complete\n",
      "Pull 96 complete\n",
      "Pull 97 complete\n",
      "Pull 98 complete\n",
      "Pull 99 complete\n"
     ]
    }
   ],
   "source": [
    "#call the function\n",
    "sub = so_fresh(100, \"prorevenge\", \"submission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def so_clean(df = sub):\n",
    "    '''Takes in a dataframe, drops duplicates and nulls, then reports the change in shape\n",
    "    default value is the sub created when so_fresh is called\n",
    "    \n",
    "    example: so_clean(df)\n",
    "    returns df after dedupe and removing nulls'''\n",
    "    df_sub= pd.concat(df)\n",
    "    print (f\"Starting dimensions: {df_sub.shape}\")\n",
    "    #drop duplicate rows based on values in \"selftext\"\n",
    "    #this removes all the posts that have been deleted\n",
    "    #or removed\n",
    "    df_sub.drop_duplicates([\"selftext\"], keep = False, inplace = True)\n",
    "    #drop nulls\n",
    "    df_sub.dropna(how = \"any\", inplace = True)\n",
    "    \n",
    "    \n",
    "    return df_sub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dimensions: (10000, 3)\n",
      "Ending dimensions: (5883, 3)\n",
      "Ending dimensions: (5883, 3)\n"
     ]
    }
   ],
   "source": [
    "df_prorevenge = so_clean(sub)\n",
    "df_prorevenge = df_prorevenge[df_prorevenge[\"subreddit\"] == \"ProRevenge\"]\n",
    "print (f\"Ending dimensions: {df_prorevenge.shape}\")\n",
    "df_prorevenge.to_csv(r'prorevenge.csv',index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stantokenia(df):\n",
    "    '''Function accepts dataframe. Tokenizes contents of the \"selftext\" column. Returns dataframe'''\n",
    "    #create a column in the data frame of tokens comprised of the lowercase components of selftext\n",
    "    #reddit text is similar to tweet text as opposed to standard speech\n",
    "    #used TweetTokenizer to split my strings-http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.casual\n",
    "    tokenizer = TweetTokenizer()\n",
    "    df[\"tokens\"] = df[\"selftext\"].apply(tokenizer.tokenize)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prorevenge = stantokenia(df_prorevenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I met a guy online in May on a student forum. ...</td>\n",
       "      <td>ProRevenge</td>\n",
       "      <td>1600797165</td>\n",
       "      <td>[I, met, a, guy, online, in, May, on, a, stude...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a story in the making. My gf recently ...</td>\n",
       "      <td>ProRevenge</td>\n",
       "      <td>1600757538</td>\n",
       "      <td>[This, is, a, story, in, the, making, ., My, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>My now ex-husband, Sean (not real name), came ...</td>\n",
       "      <td>ProRevenge</td>\n",
       "      <td>1600738776</td>\n",
       "      <td>[My, now, ex-husband, ,, Sean, (, not, real, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>It’s bullshit, how will bully’s learn if there...</td>\n",
       "      <td>ProRevenge</td>\n",
       "      <td>1600712335</td>\n",
       "      <td>[It, ’, s, bullshit, ,, how, will, bully, ’, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Not mine but a friend's story from today this ...</td>\n",
       "      <td>ProRevenge</td>\n",
       "      <td>1600710776</td>\n",
       "      <td>[Not, mine, but, a, friend's, story, from, tod...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            selftext   subreddit  created_utc  \\\n",
       "1  I met a guy online in May on a student forum. ...  ProRevenge   1600797165   \n",
       "2  This is a story in the making. My gf recently ...  ProRevenge   1600757538   \n",
       "6  My now ex-husband, Sean (not real name), came ...  ProRevenge   1600738776   \n",
       "8  It’s bullshit, how will bully’s learn if there...  ProRevenge   1600712335   \n",
       "9  Not mine but a friend's story from today this ...  ProRevenge   1600710776   \n",
       "\n",
       "                                              tokens  \n",
       "1  [I, met, a, guy, online, in, May, on, a, stude...  \n",
       "2  [This, is, a, story, in, the, making, ., My, g...  \n",
       "6  [My, now, ex-husband, ,, Sean, (, not, real, n...  \n",
       "8  [It, ’, s, bullshit, ,, how, will, bully, ’, s...  \n",
       "9  [Not, mine, but, a, friend's, story, from, tod...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prorevenge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosa_parks(text):\n",
    "    '''Lemmatizer. Accepts text, returns list'''\n",
    "    lemmatizer = lemma()\n",
    "    return [lemmatizer.lemmatize(w) for w in text] #from stack overflow, create function to apply to data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roses(df):\n",
    "    '''Function accepts dataframe. Applies lemmatizer to create \"lemmas\" and filters through stopwords to create\n",
    "    \"unique\". Returns dataframe'''\n",
    "    \n",
    "    df[\"lemmas\"]= df[\"tokens\"].apply(rosa_parks)\n",
    "    df[\"unique\"] = df[\"lemmas\"].apply(lambda x: [word for word in x if word not in (stop)])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prorevenge = roses(df_prorevenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>unique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I met a guy online in May on a student forum. ...</td>\n",
       "      <td>ProRevenge</td>\n",
       "      <td>1600797165</td>\n",
       "      <td>[I, met, a, guy, online, in, May, on, a, stude...</td>\n",
       "      <td>[I, met, a, guy, online, in, May, on, a, stude...</td>\n",
       "      <td>[I, met, guy, online, May, student, forum, ., ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a story in the making. My gf recently ...</td>\n",
       "      <td>ProRevenge</td>\n",
       "      <td>1600757538</td>\n",
       "      <td>[This, is, a, story, in, the, making, ., My, g...</td>\n",
       "      <td>[This, is, a, story, in, the, making, ., My, g...</td>\n",
       "      <td>[This, story, making, ., My, gf, recently, gav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>My now ex-husband, Sean (not real name), came ...</td>\n",
       "      <td>ProRevenge</td>\n",
       "      <td>1600738776</td>\n",
       "      <td>[My, now, ex-husband, ,, Sean, (, not, real, n...</td>\n",
       "      <td>[My, now, ex-husband, ,, Sean, (, not, real, n...</td>\n",
       "      <td>[My, ex-husband, ,, Sean, (, real, name, ), ,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>It’s bullshit, how will bully’s learn if there...</td>\n",
       "      <td>ProRevenge</td>\n",
       "      <td>1600712335</td>\n",
       "      <td>[It, ’, s, bullshit, ,, how, will, bully, ’, s...</td>\n",
       "      <td>[It, ’, s, bullshit, ,, how, will, bully, ’, s...</td>\n",
       "      <td>[It, ’, bullshit, ,, bully, ’, learn, ’, conse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Not mine but a friend's story from today this ...</td>\n",
       "      <td>ProRevenge</td>\n",
       "      <td>1600710776</td>\n",
       "      <td>[Not, mine, but, a, friend's, story, from, tod...</td>\n",
       "      <td>[Not, mine, but, a, friend's, story, from, tod...</td>\n",
       "      <td>[Not, mine, friend's, story, today, much, alot...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            selftext   subreddit  created_utc  \\\n",
       "1  I met a guy online in May on a student forum. ...  ProRevenge   1600797165   \n",
       "2  This is a story in the making. My gf recently ...  ProRevenge   1600757538   \n",
       "6  My now ex-husband, Sean (not real name), came ...  ProRevenge   1600738776   \n",
       "8  It’s bullshit, how will bully’s learn if there...  ProRevenge   1600712335   \n",
       "9  Not mine but a friend's story from today this ...  ProRevenge   1600710776   \n",
       "\n",
       "                                              tokens  \\\n",
       "1  [I, met, a, guy, online, in, May, on, a, stude...   \n",
       "2  [This, is, a, story, in, the, making, ., My, g...   \n",
       "6  [My, now, ex-husband, ,, Sean, (, not, real, n...   \n",
       "8  [It, ’, s, bullshit, ,, how, will, bully, ’, s...   \n",
       "9  [Not, mine, but, a, friend's, story, from, tod...   \n",
       "\n",
       "                                              lemmas  \\\n",
       "1  [I, met, a, guy, online, in, May, on, a, stude...   \n",
       "2  [This, is, a, story, in, the, making, ., My, g...   \n",
       "6  [My, now, ex-husband, ,, Sean, (, not, real, n...   \n",
       "8  [It, ’, s, bullshit, ,, how, will, bully, ’, s...   \n",
       "9  [Not, mine, but, a, friend's, story, from, tod...   \n",
       "\n",
       "                                              unique  \n",
       "1  [I, met, guy, online, May, student, forum, ., ...  \n",
       "2  [This, story, making, ., My, gf, recently, gav...  \n",
       "6  [My, ex-husband, ,, Sean, (, real, name, ), ,,...  \n",
       "8  [It, ’, bullshit, ,, bully, ’, learn, ’, conse...  \n",
       "9  [Not, mine, friend's, story, today, much, alot...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prorevenge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of the top 500 tokens in ProRevenge. Export the list of words for use in [Notebook 1](./Tiffany_Baker_Project_3_Reddit_Notebook_1) to create custom\n",
    "stopwords list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_list = []  # list containing all words of all texts\n",
    "for elmnt in df_prorevenge['unique']:  # loop over lists in df\n",
    "    full_list += elmnt  # append elements of lists to full list\n",
    "\n",
    "val_counts = pd.Series(full_list).value_counts()  # make temporary Series to count\n",
    "\n",
    "pro_top_500_count = val_counts.head(500).astype(int).tolist()\n",
    "pro_top_500_index = val_counts.head(500).index.tolist()\n",
    "\n",
    "pro_top_500 = list(zip(pro_top_500_index, pro_top_500_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'pro_top_500' (list)\n",
      "Stored 'pro_top_500_index' (list)\n"
     ]
    }
   ],
   "source": [
    "%store pro_top_500\n",
    "%store pro_top_500_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import custom stop word list from [Notebook 1](./Tiffany_Baker_Project_3_Reddit_Notebook_1). Clean tokens using this stopword list using the remix function.\n",
    "Export dataframe for use in [Notebook 1](./Tiffany_Baker_Project_3_Reddit_Notebook_1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r top_of_the_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remix(df):\n",
    "    '''Function to apply custom stop word list (\"top_of_the_top\") to tokens. \n",
    "    Takes dataframe, returns dataframe. Adds column \"custom\" that removes tokens in \"unique\" that are \n",
    "    in custom stop words list.'''\n",
    "    \n",
    "    df[\"custom\"] = df[\"unique\"].apply(lambda x: [word for word in x if word not in (top_of_the_top)])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'df_prorevenge' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "df_prorevenge = remix(df_prorevenge)\n",
    "%store df_prorevenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
